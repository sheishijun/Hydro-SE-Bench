"""
ç¤ºä¾‹ 5: å®Œæ•´å·¥ä½œæµç¨‹
å±•ç¤ºä»Žæ•°æ®éªŒè¯ã€é‡‡æ ·ã€è¯„ä¼°åˆ°æ·±åº¦åˆ†æžçš„å®Œæ•´ä¼ä¸šçº§å·¥ä½œæµç¨‹
åŒ…æ‹¬ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ã€é‡‡æ ·åˆ›å»ºå­é›†ã€æ‰¹é‡è¯„ä¼°ã€è¯¦ç»†åˆ†æžã€é”™è¯¯åˆ†æžã€æŠ¥å‘Šç”Ÿæˆç­‰
"""

from utils import setup_package_path, get_output_dir, get_test_data_path
from pathlib import Path

# è®¾ç½®åŒ…è·¯å¾„
PROJECT_ROOT = setup_package_path()

from hydrobench import (
    evaluate_all_models,
    load_builtin_benchmark,
    create_summary_excel,
    identify_model_columns,
    sample_benchmark_by_category,
    validate_data_quality,
    generate_analysis_report,
)
from hydrobench.excel_loader import _read_csv_safe, _detect_file_format
import pandas as pd


def main():
    """å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹"""
    print("=" * 80)
    print("ç¤ºä¾‹ 5: å®Œæ•´å·¥ä½œæµç¨‹")
    print("=" * 80)
    print("æœ¬ç¤ºä¾‹å±•ç¤ºï¼šæ•°æ®éªŒè¯ â†’ é‡‡æ · â†’ è¯„ä¼° â†’ æ·±åº¦åˆ†æž â†’ æŠ¥å‘Šç”Ÿæˆ")
    print("=" * 80)
    print()
    
    # ========== é˜¶æ®µ 1: æ•°æ®å‡†å¤‡å’ŒéªŒè¯ ==========
    print("ã€é˜¶æ®µ 1ã€‘æ•°æ®å‡†å¤‡å’ŒéªŒè¯")
    print("-" * 80)
    
    excel_path = get_test_data_path(PROJECT_ROOT)
    output_dir = get_output_dir(PROJECT_ROOT, "example_5_complete_workflow")
    
    if not excel_path.exists():
        print(f"âš  ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        print("è¯·ç¡®ä¿ test.xlsx æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–ä¿®æ”¹ excel_path æŒ‡å‘æ‚¨çš„æ•°æ®æ–‡ä»¶")
        return
    
    print(f"è¾“å…¥æ–‡ä»¶: {excel_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    # 1.1 åŠ è½½æµ‹è¯„é›†
    print("æ­¥éª¤ 1.1: åŠ è½½æµ‹è¯„é›†...")
    benchmark = load_builtin_benchmark("hydrobench")
    print(f"âœ“ å·²åŠ è½½æµ‹è¯„é›†ï¼Œå…± {len(benchmark.examples)} é“é¢˜ç›®")
    
    # ç»Ÿè®¡æµ‹è¯„é›†ä¿¡æ¯
    category_counts = {}
    level_counts = {}
    for ex in benchmark.examples:
        category_counts[ex.category] = category_counts.get(ex.category, 0) + 1
        if ex.level:
            level_counts[ex.level] = level_counts.get(ex.level, 0) + 1
    
    print(f"  - ç±»åˆ«åˆ†å¸ƒ: {dict(sorted(category_counts.items()))}")
    if level_counts:
        print(f"  - éš¾åº¦åˆ†å¸ƒ: {dict(sorted(level_counts.items()))}")
    print()
    
    # 1.2 é¢„è§ˆå’ŒéªŒè¯æ•°æ®
    print("æ­¥éª¤ 1.2: é¢„è§ˆå’ŒéªŒè¯é¢„æµ‹æ•°æ®...")
    # æ ¹æ®æ–‡ä»¶æ ¼å¼å®‰å…¨åœ°è¯»å–æ–‡ä»¶
    file_ext = excel_path.suffix.lower()
    actual_format = _detect_file_format(excel_path)
    
    # å¦‚æžœæ‰©å±•åæ˜¯ .csv ä½†å®žé™…æ˜¯ Excel æ ¼å¼ï¼Œä½¿ç”¨ Excel è¯»å–æ–¹å¼
    if file_ext == ".csv" and actual_format == 'excel':
        print(f"  âš  è­¦å‘Š: æ–‡ä»¶æ‰©å±•åæ˜¯ .csvï¼Œä½†æ£€æµ‹åˆ°å®žé™…æ ¼å¼æ˜¯ Excelã€‚å°†ä½¿ç”¨ Excel è¯»å–æ–¹å¼ã€‚")
        df = pd.read_excel(excel_path, engine='openpyxl')
    elif file_ext == ".csv":
        df = _read_csv_safe(excel_path)
    elif file_ext in (".xlsx", ".xls"):
        # å°è¯•ä½¿ç”¨ openpyxl å¼•æ“Žï¼ˆ.xlsxï¼‰æˆ– xlrd å¼•æ“Žï¼ˆ.xlsï¼‰
        try:
            if file_ext == ".xlsx":
                df = pd.read_excel(excel_path, engine='openpyxl')
            else:
                df = pd.read_excel(excel_path, engine='xlrd')
        except Exception:
            # å¦‚æžœæŒ‡å®šå¼•æ“Žå¤±è´¥ï¼Œè®© pandas è‡ªåŠ¨é€‰æ‹©
            df = pd.read_excel(excel_path)
    else:
        # å¦‚æžœæ‰©å±•åä¸æ”¯æŒï¼Œä½†æ£€æµ‹åˆ°æ˜¯ Excel æ ¼å¼ï¼Œå°è¯•ä½¿ç”¨ Excel è¯»å–
        if actual_format == 'excel':
            print(f"  âš  è­¦å‘Š: æ–‡ä»¶æ‰©å±•åæ˜¯ {file_ext}ï¼Œä½†æ£€æµ‹åˆ°å®žé™…æ ¼å¼æ˜¯ Excelã€‚å°†å°è¯•ä½¿ç”¨ Excel è¯»å–æ–¹å¼ã€‚")
            try:
                df = pd.read_excel(excel_path, engine='openpyxl')
            except Exception:
                df = pd.read_excel(excel_path)
        else:
            raise ValueError(f"Unsupported file format: {file_ext}. Supported formats: .csv, .xlsx, .xls")
    
    print(f"  - æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"  - æ€»åˆ—æ•°: {len(df.columns)}")
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    quality_report = validate_data_quality(df, benchmark)
    print(f"  - è¯†åˆ«åˆ° {quality_report['model_count']} ä¸ªæ¨¡åž‹åˆ—")
    
    if quality_report["issues"]:
        print("  âš  å‘çŽ°ä¸¥é‡é—®é¢˜:")
        for issue in quality_report["issues"]:
            print(f"    - {issue}")
    
    if quality_report["warnings"]:
        print("  âš  è­¦å‘Šä¿¡æ¯:")
        for warning in quality_report["warnings"]:
            print(f"    - {warning}")
    
    if not quality_report["issues"]:
        print("  âœ“ æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
    print()
    
    # ========== é˜¶æ®µ 2: é‡‡æ ·åˆ›å»ºå­é›†ï¼ˆå¯é€‰ï¼‰ ==========
    print("ã€é˜¶æ®µ 2ã€‘é‡‡æ ·åˆ›å»ºæµ‹è¯•å­é›†ï¼ˆå¯é€‰ï¼‰")
    print("-" * 80)
    
    use_sampled = False  # å¯ä»¥è®¾ç½®ä¸º True æ¥ä½¿ç”¨é‡‡æ ·åŽçš„å­é›†
    if use_sampled:
        print("æ­¥éª¤ 2.1: åˆ›å»ºé‡‡æ ·å­é›†...")
        sampled_benchmark = sample_benchmark_by_category(
            benchmark,
            per_category=5,
            seed=42,
            output_path=output_dir / "sampled_benchmark.json",
        )
        benchmark = sampled_benchmark
        print(f"âœ“ å·²åˆ›å»ºé‡‡æ ·å­é›†ï¼Œå…± {len(benchmark.examples)} é“é¢˜ç›®")
        print()
    else:
        print("  (è·³è¿‡é‡‡æ ·ï¼Œä½¿ç”¨å®Œæ•´æµ‹è¯„é›†)")
        print()
    
    # ========== é˜¶æ®µ 3: æ‰¹é‡è¯„ä¼° ==========
    print("ã€é˜¶æ®µ 3ã€‘æ‰¹é‡è¯„ä¼°æ‰€æœ‰æ¨¡åž‹")
    print("-" * 80)
    
    print("æ­¥éª¤ 3.1: è¯†åˆ«æ¨¡åž‹åˆ—...")
    model_columns = identify_model_columns(df, verbose=True)
    print()
    
    if not model_columns:
        print("âš  æœªè¯†åˆ«åˆ°ä»»ä½•æ¨¡åž‹åˆ—ï¼Œè¯·æ£€æŸ¥ Excel æ–‡ä»¶æ ¼å¼")
        return
    
    print("æ­¥éª¤ 3.2: æ‰§è¡Œæ‰¹é‡è¯„ä¼°...")
    summary = evaluate_all_models(
        excel_path,
        benchmark=benchmark,
        output_dir=output_dir,
        verbose=True,
    )
    print()
    
    # ========== é˜¶æ®µ 4: ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š ==========
    print("ã€é˜¶æ®µ 4ã€‘ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    print("-" * 80)
    
    print("æ­¥éª¤ 4.1: ç”Ÿæˆæ¨¡åž‹å¯¹æ¯” Excel...")
    create_summary_excel(summary, output_dir, benchmark)
    print("âœ“ æ¨¡åž‹å¯¹æ¯”æ±‡æ€» Excel å·²ç”Ÿæˆ")
    print()
    
    # ========== é˜¶æ®µ 5: æ·±åº¦åˆ†æž ==========
    print("ã€é˜¶æ®µ 5ã€‘æ·±åº¦æ•°æ®åˆ†æž")
    print("-" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æžœ
    if not summary["results"]:
        print("âš  æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡åž‹ï¼Œæ— æ³•è¿›è¡Œæ·±åº¦åˆ†æž")
        if summary.get("errors"):
            print("\nå¤„ç†å¤±è´¥çš„æ¨¡åž‹:")
            for error in summary["errors"]:
                print(f"  - {error['model_name']}: {error['error']}")
        return
    
    # 5.1 åŸºç¡€ç»Ÿè®¡åˆ†æž
    print("æ­¥éª¤ 5.1: åŸºç¡€ç»Ÿè®¡åˆ†æž...")
    best_model = summary["results"][0]
    worst_model = summary["results"][-1]
    avg_accuracy = sum(r["accuracy"] for r in summary["results"]) / len(summary["results"])
    
    print(f"  ðŸ† æœ€ä½³æ¨¡åž‹: {best_model['model_name']} ({best_model['accuracy']:.2%})")
    print(f"  ðŸ“‰ æœ€å·®æ¨¡åž‹: {worst_model['model_name']} ({worst_model['accuracy']:.2%})")
    print(f"  ðŸ“Š å¹³å‡å‡†ç¡®çŽ‡: {avg_accuracy:.2%}")
    print(f"  ðŸ“ˆ å‡†ç¡®çŽ‡æ ‡å‡†å·®: {pd.Series([r['accuracy'] for r in summary['results']]).std():.4f}")
    print()
    
    # 5.2 ç±»åˆ«åˆ†æž
    print("æ­¥éª¤ 5.2: ç±»åˆ«ç»´åº¦åˆ†æž...")
    if any(r.get("category_stats") for r in summary["results"]):
        all_categories = set()
        for result in summary["results"]:
            if result.get("category_stats"):
                all_categories.update(result["category_stats"].keys())
        
        print("  å„ç±»åˆ«æœ€ä½³è¡¨çŽ°:")
        for category in sorted(all_categories):
            best_acc = 0
            best_model_name = ""
            worst_acc = 1.0
            worst_model_name = ""
            
            for result in summary["results"]:
                stats = result.get("category_stats", {}).get(category, {})
                if stats:
                    acc = stats.get("accuracy", 0)
                    if acc > best_acc:
                        best_acc = acc
                        best_model_name = result["model_name"]
                    if acc < worst_acc:
                        worst_acc = acc
                        worst_model_name = result["model_name"]
            
            print(f"    {category}:")
            print(f"      æœ€ä½³: {best_model_name} ({best_acc:.2%})")
            print(f"      æœ€å·®: {worst_model_name} ({worst_acc:.2%})")
            print(f"      å·®è·: {(best_acc - worst_acc):.2%}")
    print()
    
    # 5.3 éš¾åº¦åˆ†æž
    print("æ­¥éª¤ 5.3: éš¾åº¦ç»´åº¦åˆ†æž...")
    if any(r.get("level_stats") for r in summary["results"]):
        all_levels = set()
        for result in summary["results"]:
            if result.get("level_stats"):
                all_levels.update(result["level_stats"].keys())
        
        print("  å„éš¾åº¦çº§åˆ«å¹³å‡è¡¨çŽ°:")
        for level in sorted(all_levels):
            level_accuracies = []
            for result in summary["results"]:
                stats = result.get("level_stats", {}).get(level, {})
                if stats:
                    level_accuracies.append(stats.get("accuracy", 0))
            
            if level_accuracies:
                avg_level_acc = sum(level_accuracies) / len(level_accuracies)
                print(f"    {level}: å¹³å‡å‡†ç¡®çŽ‡ {avg_level_acc:.2%} ({len(level_accuracies)} ä¸ªæ¨¡åž‹)")
    print()
    
    
    # 5.4 ç”Ÿæˆæ·±åº¦åˆ†æžæŠ¥å‘Š
    print("æ­¥éª¤ 5.5: ç”Ÿæˆæ·±åº¦åˆ†æžæŠ¥å‘Š...")
    analysis_report = generate_analysis_report(summary, benchmark, output_dir)
    print()
    
    # ========== é˜¶æ®µ 6: æ€»ç»“å’Œå»ºè®® ==========
    print("ã€é˜¶æ®µ 6ã€‘æ€»ç»“å’Œå»ºè®®")
    print("-" * 80)
    
    print("è¯„ä¼°æ€»ç»“:")
    print(f"  - å…±è¯„ä¼° {summary['models_count']} ä¸ªæ¨¡åž‹")
    print(f"  - å…± {summary['total_questions']} é“é¢˜ç›®")
    print(f"  - æœ€ä½³æ¨¡åž‹å‡†ç¡®çŽ‡: {best_model['accuracy']:.2%}")
    print(f"  - æ¨¡åž‹é—´å‡†ç¡®çŽ‡å·®è·: {(best_model['accuracy'] - worst_model['accuracy']):.2%}")
    print()
    
    print("å…³é”®å»ºè®®:")
    for i, rec in enumerate(analysis_report["recommendations"], 1):
        print(f"  {i}. {rec}")
    print()
    
    # ========== è¾“å‡ºæ–‡ä»¶æ¸…å• ==========
    print("=" * 80)
    print("ç”Ÿæˆçš„æ–‡ä»¶æ¸…å•")
    print("=" * 80)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    print("\næ–‡ä»¶åˆ—è¡¨:")
    
    file_count = 0
    total_size = 0
    for file in sorted(output_dir.glob("*")):
        if file.is_file():
            size = file.stat().st_size
            total_size += size
            size_str = f"{size / 1024:.1f} KB" if size < 1024 * 1024 else f"{size / (1024 * 1024):.1f} MB"
            print(f"  - {file.name} ({size_str})")
            file_count += 1
    
    print(f"\næ€»è®¡: {file_count} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {total_size / (1024 * 1024):.2f} MB")
    print("\n" + "=" * 80)
    print("âœ“ å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()

