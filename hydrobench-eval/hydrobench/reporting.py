"""
æ±‡æ€»æŠ¥å‘Šç”ŸæˆåŠŸèƒ½
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

try:
    import pandas as pd
    import numpy as np
except ImportError:
    pd = None  # type: ignore[assignment]
    np = None  # type: ignore[assignment]

from .batch_evaluate import identify_model_columns
from .benchmark import Benchmark


def _generate_word_report(md_lines: list[str], output_dir: Path) -> None:
    """å°† Markdown å†…å®¹è½¬æ¢ä¸º Word æ–‡æ¡£"""
    try:
        from docx import Document
        from docx.shared import Pt, Inches, RGBColor
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        import re
    except ImportError:
        print("  âš  python-docx æœªå®‰è£…ï¼Œè·³è¿‡ Word æŠ¥å‘Šç”Ÿæˆ")
        print("    æç¤º: è¿è¡Œ pip install python-docx å¯ç”Ÿæˆ Word æ ¼å¼æŠ¥å‘Š")
        return
    
    try:
        doc = Document()
        
        # è®¾ç½®é»˜è®¤å­—ä½“
        style = doc.styles['Normal']
        font = style.font
        font.name = 'Microsoft YaHei'  # ä¸­æ–‡å­—ä½“
        font.size = Pt(10.5)
        
        i = 0
        while i < len(md_lines):
            line = md_lines[i].strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                i += 1
                continue
            
            # å›¾ç‰‡å¤„ç†ï¼ˆMarkdown æ ¼å¼ï¼š![alt](path)ï¼‰
            if line.startswith('![') and '](' in line:
                # æå–å›¾ç‰‡è·¯å¾„
                try:
                    img_path = line.split('](')[1].split(')')[0]
                    img_file = output_dir / img_path
                    if img_file.exists() and img_file.is_file():
                        doc.add_picture(str(img_file), width=Inches(6))
                        doc.add_paragraph()  # æ·»åŠ ç©ºè¡Œ
                except Exception:
                    pass  # å¦‚æœå›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡
                i += 1
                continue
            
            # æ ‡é¢˜å¤„ç†
            if line.startswith('# '):
                # ä¸€çº§æ ‡é¢˜
                p = doc.add_heading(line[2:], level=1)
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
            elif line.startswith('## '):
                # äºŒçº§æ ‡é¢˜
                p = doc.add_heading(line[3:], level=2)
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
            elif line.startswith('### '):
                # ä¸‰çº§æ ‡é¢˜
                p = doc.add_heading(line[4:], level=3)
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
            elif line.startswith('---'):
                # åˆ†éš”çº¿
                doc.add_paragraph('â”€' * 50)
            elif line.startswith('|'):
                # è¡¨æ ¼
                table_lines = []
                while i < len(md_lines) and md_lines[i].strip().startswith('|'):
                    table_lines.append(md_lines[i].strip())
                    i += 1
                i -= 1  # å›é€€ä¸€æ­¥
                
                if len(table_lines) >= 2:
                    # è§£æè¡¨å¤´
                    headers = [cell.strip() for cell in table_lines[0].split('|')[1:-1]]
                    # è·³è¿‡åˆ†éš”è¡Œ
                    data_rows = table_lines[2:]
                    
                    # åˆ›å»ºè¡¨æ ¼
                    table = doc.add_table(rows=1, cols=len(headers))
                    table.style = 'Light Grid Accent 1'
                    
                    # æ·»åŠ è¡¨å¤´
                    header_cells = table.rows[0].cells
                    for j, header in enumerate(headers):
                        header_cells[j].text = header
                        header_cells[j].paragraphs[0].runs[0].font.bold = True
                    
                    # æ·»åŠ æ•°æ®è¡Œ
                    for row_line in data_rows:
                        if row_line.strip().startswith('| ...'):
                            continue  # è·³è¿‡çœç•¥è¡Œ
                        cells = [cell.strip() for cell in row_line.split('|')[1:-1]]
                        if len(cells) == len(headers):
                            row_cells = table.add_row().cells
                            for j, cell_text in enumerate(cells):
                                row_cells[j].text = cell_text
            elif line.startswith('- ') or line.startswith('* '):
                # åˆ—è¡¨é¡¹
                text = line[2:].strip()
                # å¤„ç†ç²—ä½“
                text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
                if text:  # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
                    doc.add_paragraph(text, style='List Bullet')
            elif re.match(r'^\d+\.\s+', line):
                # æœ‰åºåˆ—è¡¨ï¼ˆåŒ¹é…ä»»ä½•æ•°å­—å¼€å¤´çš„åˆ—è¡¨é¡¹ï¼‰
                text = re.sub(r'^\d+\.\s+', '', line)
                # å¤„ç†ç²—ä½“
                text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
                if text:  # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
                    doc.add_paragraph(text, style='List Number')
            else:
                # æ™®é€šæ®µè½
                # å¤„ç†ç²—ä½“
                text = re.sub(r'\*\*(.+?)\*\*', r'\1', line)
                if text:  # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
                    doc.add_paragraph(text)
            
            i += 1
        
        # ä¿å­˜ Word æ–‡æ¡£
        word_file = output_dir / "detailed_analysis_report.docx"
        doc.save(str(word_file))
        print(f"âœ“ æ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆWordï¼‰å·²ä¿å­˜: {word_file}")
    except Exception as e:
        print(f"  âš  Word æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")


def _generate_pdf_report(md_lines: list[str], output_dir: Path) -> None:
    """å°† Markdown å†…å®¹è½¬æ¢ä¸º PDF æ–‡æ¡£"""
    try:
        import markdown
    except ImportError:
        print("  âš  markdown æœªå®‰è£…ï¼Œè·³è¿‡ PDF æŠ¥å‘Šç”Ÿæˆ")
        print("    æç¤º: è¿è¡Œ pip install markdown å¯ç”Ÿæˆ PDF æ ¼å¼æŠ¥å‘Š")
        print("    æˆ–è€…: ä½¿ç”¨ Word æ–‡æ¡£åœ¨ Microsoft Word ä¸­å¦å­˜ä¸º PDF")
        return
    
    try:
        from weasyprint import HTML, CSS
        from weasyprint.text.fonts import FontConfiguration
    except (ImportError, OSError) as e:
        print("  âš  weasyprint æœªå®‰è£…æˆ–ç³»ç»Ÿä¾èµ–ç¼ºå¤±ï¼Œè·³è¿‡ PDF æŠ¥å‘Šç”Ÿæˆ")
        print(f"    é”™è¯¯: {e}")
        print("    æç¤º: weasyprint åœ¨ Windows ä¸Šéœ€è¦é¢å¤–çš„ç³»ç»Ÿåº“ï¼Œå®‰è£…è¾ƒå¤æ‚")
        print("    å»ºè®®: ä½¿ç”¨ Word æ–‡æ¡£åœ¨ Microsoft Word ä¸­å¦å­˜ä¸º PDF")
        return
    
    try:
        # å°† Markdown è½¬æ¢ä¸º HTML
        md_content = "\n".join(md_lines)
        html_content = markdown.markdown(
            md_content,
            extensions=['tables', 'fenced_code']
        )
        
        # æ·»åŠ æ ·å¼
        styled_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                @page {{
                    size: A4;
                    margin: 2cm;
                }}
                body {{
                    font-family: "Microsoft YaHei", "SimSun", Arial, sans-serif;
                    font-size: 11pt;
                    line-height: 1.6;
                    color: #333;
                }}
                h1 {{
                    font-size: 24pt;
                    color: #2c3e50;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                    margin-top: 30px;
                }}
                h2 {{
                    font-size: 18pt;
                    color: #34495e;
                    margin-top: 25px;
                    border-bottom: 2px solid #ecf0f1;
                    padding-bottom: 5px;
                }}
                h3 {{
                    font-size: 14pt;
                    color: #7f8c8d;
                    margin-top: 20px;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 15px 0;
                    font-size: 10pt;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 8px;
                    text-align: left;
                }}
                th {{
                    background-color: #3498db;
                    color: white;
                    font-weight: bold;
                }}
                tr:nth-child(even) {{
                    background-color: #f9f9f9;
                }}
                ul, ol {{
                    margin: 10px 0;
                    padding-left: 30px;
                }}
                li {{
                    margin: 5px 0;
                }}
                p {{
                    margin: 10px 0;
                }}
            </style>
        </head>
        <body>
            {html_content}
        </body>
        </html>
        """
        
        # ç”Ÿæˆ PDF
        pdf_file = output_dir / "detailed_analysis_report.pdf"
        font_config = FontConfiguration()
        HTML(string=styled_html).write_pdf(
            pdf_file,
            font_config=font_config
        )
        print(f"âœ“ æ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆPDFï¼‰å·²ä¿å­˜: {pdf_file}")
    except (OSError, Exception) as e:
        print(f"  âš  PDF æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        print("    æç¤º: weasyprint åœ¨ Windows ä¸Šå¯èƒ½éœ€è¦é¢å¤–çš„ç³»ç»Ÿåº“")
        print("    å»ºè®®: ä½¿ç”¨ Word æ–‡æ¡£åœ¨ Microsoft Word ä¸­å¦å­˜ä¸º PDF")


def create_summary_excel(summary: dict[str, Any], output_dir: Path, benchmark: Any = None) -> None:
    """
    åˆ›å»ºæ±‡æ€» Excel æŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”å’ŒæŒ‰ç±»åˆ«/éš¾åº¦çš„ç»Ÿè®¡ã€‚
    
    Args:
        summary: è¯„ä¼°æ±‡æ€»ç»“æœå­—å…¸ï¼ŒåŒ…å« resultsã€benchmarkã€total_questions ç­‰å­—æ®µ
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        benchmark: Benchmark å¯¹è±¡ï¼ˆå¯é€‰ï¼Œå½“å‰æœªä½¿ç”¨ä½†ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
    """
    if pd is None:
        print("âš  pandas æœªå®‰è£…ï¼Œè·³è¿‡æ±‡æ€» Excel æŠ¥å‘Šç”Ÿæˆ")
        return
    
    # æ¨¡å‹å¯¹æ¯”è¡¨
    rows = []
    for result in summary["results"]:
        rows.append({
            "Model Name": result["model_name"],
            "Correct Count": result["total_score"],
            "Total": result["max_score"],
            "Accuracy": result["accuracy"],  # ä½¿ç”¨æ•°å€¼ï¼ŒExcelä¼šè‡ªåŠ¨æ ¼å¼åŒ–ä¸ºç™¾åˆ†æ¯”
            "Incorrect": result["incorrect_count"],
            "Missing": result["missing_count"],
        })
    
    df = pd.DataFrame(rows)
    
    summary_file = output_dir / "models_comparison.xlsx"
    with pd.ExcelWriter(summary_file, engine="openpyxl") as writer:
        # æ¨¡å‹å¯¹æ¯”å·¥ä½œè¡¨
        df.to_excel(writer, sheet_name="Model Comparison", index=False)
        worksheet = writer.sheets["Model Comparison"]
        worksheet.column_dimensions["A"].width = 40  # Model Name
        worksheet.column_dimensions["B"].width = 12  # Correct Count
        worksheet.column_dimensions["C"].width = 12  # Total
        worksheet.column_dimensions["D"].width = 12  # Accuracy
        worksheet.column_dimensions["E"].width = 10  # Incorrect
        worksheet.column_dimensions["F"].width = 10  # Missing
        
        # è®¾ç½®å‡†ç¡®ç‡åˆ—ä¸ºç™¾åˆ†æ¯”æ ¼å¼
        from openpyxl.styles import numbers
        for row in range(2, len(df) + 2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
            cell = worksheet.cell(row, 4)  # Dåˆ—æ˜¯å‡†ç¡®ç‡
            cell.number_format = numbers.FORMAT_PERCENTAGE_00  # ç™¾åˆ†æ¯”æ ¼å¼ï¼Œä¿ç•™2ä½å°æ•°
        
        # æ·»åŠ æ±‡æ€»ä¿¡æ¯
        summary_row = len(df) + 3
        worksheet.cell(summary_row, 1, "Benchmark Info")
        worksheet.cell(summary_row + 1, 1, "Benchmark: hydrobench")
        worksheet.cell(summary_row + 2, 1, f"Total Questions: {summary['total_questions']}")
        worksheet.cell(summary_row + 3, 1, f"Models Evaluated: {summary['models_count']}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡ï¼ˆæ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”ï¼‰
        if any(result.get("category_stats") for result in summary["results"]):
            # æ”¶é›†æ‰€æœ‰ç±»åˆ«
            all_categories = set()
            for result in summary["results"]:
                if result.get("category_stats"):
                    all_categories.update(result["category_stats"].keys())
            
            if all_categories:
                category_rows = []
                for category in sorted(all_categories):
                    row = {"Category": category}
                    for result in summary["results"]:
                        model_name = result["model_name"]
                        stats = result.get("category_stats", {}).get(category, {})
                        if stats:
                            # åˆ†å¼€ä¸¤åˆ—ï¼šå¾—åˆ†å’Œå‡†ç¡®ç‡
                            row[f"{model_name}_Score"] = f"{stats.get('correct', 0)}/{stats.get('total', 0)}"
                            row[f"{model_name}_Accuracy"] = stats.get('accuracy', 0)  # æ•°å€¼æ ¼å¼
                        else:
                            row[f"{model_name}_Score"] = "-"
                            row[f"{model_name}_Accuracy"] = None
                    category_rows.append(row)
                
                category_df = pd.DataFrame(category_rows)
                category_df.to_excel(writer, sheet_name="By Category Comparison", index=False)
                cat_worksheet = writer.sheets["By Category Comparison"]
                cat_worksheet.column_dimensions["A"].width = 30  # Category
                
                # è®¾ç½®åˆ—å®½å’Œæ ¼å¼
                col_idx = 2
                for result in summary["results"]:
                    model_name = result["model_name"]
                    # å¾—åˆ†åˆ—
                    cat_worksheet.column_dimensions[chr(64 + col_idx)].width = 20
                    col_idx += 1
                    # å‡†ç¡®ç‡åˆ— - è®¾ç½®ä¸ºç™¾åˆ†æ¯”æ ¼å¼
                    from openpyxl.styles import numbers
                    for row in range(2, len(category_df) + 2):
                        cell = cat_worksheet.cell(row, col_idx)
                        if cell.value is not None:
                            cell.number_format = numbers.FORMAT_PERCENTAGE_00
                    cat_worksheet.column_dimensions[chr(64 + col_idx)].width = 15
                    col_idx += 1
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡ï¼ˆæ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”ï¼‰
        if any(result.get("level_stats") for result in summary["results"]):
            # æ”¶é›†æ‰€æœ‰éš¾åº¦
            all_levels = set()
            for result in summary["results"]:
                if result.get("level_stats"):
                    all_levels.update(result["level_stats"].keys())
            
            if all_levels:
                level_rows = []
                for level in sorted(all_levels):
                    row = {"Level": level}
                    for result in summary["results"]:
                        model_name = result["model_name"]
                        stats = result.get("level_stats", {}).get(level, {})
                        if stats:
                            # åˆ†å¼€ä¸¤åˆ—ï¼šå¾—åˆ†å’Œå‡†ç¡®ç‡
                            row[f"{model_name}_Score"] = f"{stats.get('correct', 0)}/{stats.get('total', 0)}"
                            row[f"{model_name}_Accuracy"] = stats.get('accuracy', 0)  # æ•°å€¼æ ¼å¼
                        else:
                            row[f"{model_name}_Score"] = "-"
                            row[f"{model_name}_Accuracy"] = None
                    level_rows.append(row)
                
                level_df = pd.DataFrame(level_rows)
                level_df.to_excel(writer, sheet_name="By Level Comparison", index=False)
                level_worksheet = writer.sheets["By Level Comparison"]
                level_worksheet.column_dimensions["A"].width = 30  # Level
                
                # è®¾ç½®åˆ—å®½å’Œæ ¼å¼
                col_idx = 2
                for result in summary["results"]:
                    model_name = result["model_name"]
                    # å¾—åˆ†åˆ—
                    level_worksheet.column_dimensions[chr(64 + col_idx)].width = 20
                    col_idx += 1
                    # å‡†ç¡®ç‡åˆ— - è®¾ç½®ä¸ºç™¾åˆ†æ¯”æ ¼å¼
                    from openpyxl.styles import numbers
                    for row in range(2, len(level_df) + 2):
                        cell = level_worksheet.cell(row, col_idx)
                        if cell.value is not None:
                            cell.number_format = numbers.FORMAT_PERCENTAGE_00
                    level_worksheet.column_dimensions[chr(64 + col_idx)].width = 15
                    col_idx += 1
        
        # æŒ‰é¢˜å‹ç»Ÿè®¡ï¼ˆæ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”ï¼‰
        if any(result.get("type_stats") for result in summary["results"]):
            # æ”¶é›†æ‰€æœ‰é¢˜å‹
            all_types = set()
            for result in summary["results"]:
                if result.get("type_stats"):
                    all_types.update(result["type_stats"].keys())
            
            if all_types:
                type_rows = []
                for qtype in sorted(all_types):
                    row = {"Type": qtype}
                    for result in summary["results"]:
                        model_name = result["model_name"]
                        stats = result.get("type_stats", {}).get(qtype, {})
                        if stats:
                            # åˆ†å¼€ä¸¤åˆ—ï¼šå¾—åˆ†å’Œå‡†ç¡®ç‡
                            row[f"{model_name}_Score"] = f"{stats.get('correct', 0)}/{stats.get('total', 0)}"
                            row[f"{model_name}_Accuracy"] = stats.get('accuracy', 0)  # æ•°å€¼æ ¼å¼
                        else:
                            row[f"{model_name}_Score"] = "-"
                            row[f"{model_name}_Accuracy"] = None
                    type_rows.append(row)
                
                type_df = pd.DataFrame(type_rows)
                type_df.to_excel(writer, sheet_name="By Type Comparison", index=False)
                type_worksheet = writer.sheets["By Type Comparison"]
                type_worksheet.column_dimensions["A"].width = 30  # Type
                
                # è®¾ç½®åˆ—å®½å’Œæ ¼å¼
                col_idx = 2
                for result in summary["results"]:
                    model_name = result["model_name"]
                    # å¾—åˆ†åˆ—
                    type_worksheet.column_dimensions[chr(64 + col_idx)].width = 20
                    col_idx += 1
                    # å‡†ç¡®ç‡åˆ— - è®¾ç½®ä¸ºç™¾åˆ†æ¯”æ ¼å¼
                    from openpyxl.styles import numbers
                    for row in range(2, len(type_df) + 2):
                        cell = type_worksheet.cell(row, col_idx)
                        if cell.value is not None:
                            cell.number_format = numbers.FORMAT_PERCENTAGE_00
                    type_worksheet.column_dimensions[chr(64 + col_idx)].width = 15
                    col_idx += 1
    
    print(f"âœ“ æ¨¡å‹å¯¹æ¯”æ±‡æ€»å·²ä¿å­˜: {summary_file}")


def validate_data_quality(df: pd.DataFrame, benchmark: Benchmark) -> dict[str, Any]:
    """
    éªŒè¯æ•°æ®è´¨é‡ã€‚
    
    Args:
        df: é¢„æµ‹æ•°æ®çš„ DataFrame
        benchmark: Benchmark å¯¹è±¡
    
    Returns:
        åŒ…å«æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœçš„å­—å…¸
    """
    if pd is None:
        raise ImportError("pandas æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥")
    
    issues = []
    warnings = []
    
    # æ£€æŸ¥ ID åˆ—
    if "ID" not in df.columns:
        issues.append("ç¼ºå°‘ ID åˆ—")
    else:
        # æ£€æŸ¥ ID åŒ¹é…åº¦
        df_ids = set(df["ID"].astype(str))
        benchmark_ids = set(ex.id for ex in benchmark.examples)
        missing_ids = benchmark_ids - df_ids
        extra_ids = df_ids - benchmark_ids
        
        if missing_ids:
            warnings.append(f"æµ‹è¯„é›†ä¸­æœ‰ {len(missing_ids)} ä¸ª ID åœ¨é¢„æµ‹æ–‡ä»¶ä¸­ç¼ºå¤±")
        if extra_ids:
            warnings.append(f"é¢„æµ‹æ–‡ä»¶ä¸­æœ‰ {len(extra_ids)} ä¸ª ID ä¸åœ¨æµ‹è¯„é›†ä¸­")
    
    # æ£€æŸ¥ç©ºå€¼
    model_cols = identify_model_columns(df, verbose=False)
    for col in model_cols:
        null_count = df[col].isna().sum()
        if null_count > 0:
            warnings.append(f"æ¨¡å‹åˆ— '{col}' æœ‰ {null_count} ä¸ªç©ºå€¼")
    
    return {
        "issues": issues,
        "warnings": warnings,
        "model_count": len(model_cols),
        "data_rows": len(df),
        "benchmark_size": len(benchmark.examples),
    }


def analyze_errors(summary: dict[str, Any], benchmark: Benchmark, output_dir: Path) -> dict[str, Any]:
    """
    æ·±åº¦é”™è¯¯åˆ†æã€‚
    
    Args:
        summary: è¯„ä¼°æ±‡æ€»ç»“æœ
        benchmark: Benchmark å¯¹è±¡
        output_dir: è¾“å‡ºç›®å½•ï¼Œç”¨äºè¯»å–è¯¦ç»†æŠ¥å‘Š
    
    Returns:
        é”™è¯¯åˆ†æç»“æœå­—å…¸
    """
    error_analysis = {
        "common_errors": {},  # é¢˜ç›®ID -> é”™è¯¯æ¨¡å‹åˆ—è¡¨
        "hard_questions": [],  # æ‰€æœ‰æ¨¡å‹éƒ½é”™çš„é¢˜ç›®
        "easy_questions": [],  # æ‰€æœ‰æ¨¡å‹éƒ½å¯¹çš„é¢˜ç›®
        "model_errors": {},  # æ¨¡å‹ -> é”™è¯¯é¢˜ç›®åˆ—è¡¨
    }
    
    # æ”¶é›†æ¯é“é¢˜çš„é”™è¯¯æƒ…å†µ
    question_errors = {}  # question_id -> {correct_models: [], incorrect_models: []}
    
    for result in summary["results"]:
        model_name = result["model_name"]
        error_analysis["model_errors"][model_name] = []
        
        # è¯»å–è¯¥æ¨¡å‹çš„è¯¦ç»†æŠ¥å‘Š
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹å
        safe_name = model_name.replace("/", "_").replace("\\", "_").replace(":", "_").replace("*", "_").replace("?", "_").replace('"', "_").replace("<", "_").replace(">", "_").replace("|", "_")
        model_dir = output_dir / safe_name
        report_file = model_dir / "score_report.json"
        if report_file.exists():
            with open(report_file, "r", encoding="utf-8") as f:
                report_data = json.load(f)
            
            for example in report_data.get("examples", []):
                qid = example["id"]
                is_correct = example["is_correct"]
                
                if qid not in question_errors:
                    question_errors[qid] = {"correct": [], "incorrect": []}
                
                if is_correct:
                    question_errors[qid]["correct"].append(model_name)
                else:
                    question_errors[qid]["incorrect"].append(model_name)
                    error_analysis["model_errors"][model_name].append(qid)
    
    # åˆ†æé¢˜ç›®éš¾åº¦
    total_models = len(summary["results"])
    for qid, stats in question_errors.items():
        correct_count = len(stats["correct"])
        incorrect_count = len(stats["incorrect"])
        
        if incorrect_count == total_models:
            error_analysis["hard_questions"].append(qid)
        elif correct_count == total_models:
            error_analysis["easy_questions"].append(qid)
        
        if incorrect_count > total_models * 0.5:  # è¶…è¿‡ä¸€åŠæ¨¡å‹éƒ½é”™
            error_analysis["common_errors"][qid] = {
                "error_rate": incorrect_count / total_models,
                "incorrect_models": stats["incorrect"],
            }
    
    return error_analysis


def generate_analysis_report(
    summary: dict[str, Any],
    benchmark: Benchmark,
    output_dir: Path,
) -> dict[str, Any]:
    """
    ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚
    
    Args:
        summary: è¯„ä¼°æ±‡æ€»ç»“æœ
        benchmark: Benchmark å¯¹è±¡
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        åˆ†ææŠ¥å‘Šå­—å…¸ï¼ˆJSON æ ¼å¼ï¼‰
    """
    if pd is None:
        raise ImportError("pandas æœªå®‰è£…ï¼Œæ— æ³•ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    
    best_model = summary["results"][0]
    worst_model = summary["results"][-1]
    avg_accuracy = sum(r["accuracy"] for r in summary["results"]) / len(summary["results"])
    std_accuracy = pd.Series([r["accuracy"] for r in summary["results"]]).std()
    accuracy_gap = best_model["accuracy"] - worst_model["accuracy"]
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_lines = []
    md_lines.append("# æ·±åº¦åˆ†ææŠ¥å‘Š")
    md_lines.append("")
    md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")
    
    # 1. è¯„ä¼°æ¦‚è§ˆ
    md_lines.append("## ğŸ“Š è¯„ä¼°æ¦‚è§ˆ")
    md_lines.append("")
    md_lines.append(f"- **è¯„ä¼°æ¨¡å‹æ•°**: {summary['models_count']} ä¸ª")
    md_lines.append(f"- **é¢˜ç›®æ€»æ•°**: {summary['total_questions']} é“")
    md_lines.append(f"- **å¹³å‡å‡†ç¡®ç‡**: {avg_accuracy:.2%}")
    md_lines.append(f"- **å‡†ç¡®ç‡æ ‡å‡†å·®**: {std_accuracy:.4f}")
    md_lines.append("")
    
    # 2. æ¨¡å‹æ’å
    md_lines.append("## ğŸ† æ¨¡å‹æ’å")
    md_lines.append("")
    md_lines.append("| æ’å | æ¨¡å‹åç§° | å¾—åˆ† | å‡†ç¡®ç‡ |")
    md_lines.append("|------|---------|------|--------|")
    for idx, result in enumerate(summary["results"], 1):
        score_str = f"{result['total_score']}/{result['max_score']}"
        accuracy_str = f"{result['accuracy']:.2%}"
        md_lines.append(f"| {idx} | {result['model_name']} | {score_str} | {accuracy_str} |")
    md_lines.append("")
    
    # 3. å…³é”®æŒ‡æ ‡
    md_lines.append("## ğŸ“ˆ å…³é”®æŒ‡æ ‡")
    md_lines.append("")
    md_lines.append(f"### æœ€ä½³æ¨¡å‹")
    md_lines.append(f"- **æ¨¡å‹**: {best_model['model_name']}")
    md_lines.append(f"- **å‡†ç¡®ç‡**: {best_model['accuracy']:.2%}")
    md_lines.append(f"- **å¾—åˆ†**: {best_model['total_score']}/{best_model['max_score']}")
    md_lines.append("")
    md_lines.append(f"### æœ€å·®æ¨¡å‹")
    md_lines.append(f"- **æ¨¡å‹**: {worst_model['model_name']}")
    md_lines.append(f"- **å‡†ç¡®ç‡**: {worst_model['accuracy']:.2%}")
    md_lines.append(f"- **å¾—åˆ†**: {worst_model['total_score']}/{worst_model['max_score']}")
    md_lines.append("")
    md_lines.append(f"### æ¨¡å‹å·®è·")
    md_lines.append(f"- **å‡†ç¡®ç‡å·®è·**: {accuracy_gap:.2%}")
    md_lines.append("")
    
    # 4. ç±»åˆ«åˆ†æ
    if summary["results"] and summary["results"][0].get("category_stats"):
        md_lines.append("## ğŸ“‚ ç±»åˆ«ç»´åº¦åˆ†æ")
        md_lines.append("")
        all_categories = set()
        for result in summary["results"]:
            if result.get("category_stats"):
                all_categories.update(result["category_stats"].keys())
        
        for category in sorted(all_categories):
            md_lines.append(f"### {category}")
            md_lines.append("")
            md_lines.append("| æ¨¡å‹åç§° | å‡†ç¡®ç‡ | å¾—åˆ† |")
            md_lines.append("|---------|--------|------|")
            
            best_acc = 0
            best_model_name = ""
            worst_acc = 1.0
            worst_model_name = ""
            
            category_results = []
            for result in summary["results"]:
                stats = result.get("category_stats", {}).get(category, {})
                if stats:
                    acc = stats.get("accuracy", 0)
                    score_str = f"{stats.get('correct', 0)}/{stats.get('total', 0)}"
                    category_results.append((result["model_name"], acc, score_str))
                    if acc > best_acc:
                        best_acc = acc
                        best_model_name = result["model_name"]
                    if acc < worst_acc:
                        worst_acc = acc
                        worst_model_name = result["model_name"]
            
            # æŒ‰å‡†ç¡®ç‡æ’åº
            category_results.sort(key=lambda x: x[1], reverse=True)
            for model_name, acc, score_str in category_results:
                md_lines.append(f"| {model_name} | {acc:.2%} | {score_str} |")
            
            md_lines.append("")
            md_lines.append(f"- **æœ€ä½³**: {best_model_name} ({best_acc:.2%})")
            md_lines.append(f"- **æœ€å·®**: {worst_model_name} ({worst_acc:.2%})")
            md_lines.append(f"- **å·®è·**: {(best_acc - worst_acc):.2%}")
            md_lines.append("")
    
    # 5. éš¾åº¦åˆ†æ
    if any(r.get("level_stats") for r in summary["results"]):
        md_lines.append("## ğŸ¯ éš¾åº¦ç»´åº¦åˆ†æ")
        md_lines.append("")
        all_levels = set()
        for result in summary["results"]:
            if result.get("level_stats"):
                all_levels.update(result["level_stats"].keys())
        
        md_lines.append("| éš¾åº¦çº§åˆ« | å¹³å‡å‡†ç¡®ç‡ | æ¨¡å‹æ•° |")
        md_lines.append("|---------|-----------|--------|")
        
        for level in sorted(all_levels):
            level_accuracies = []
            for result in summary["results"]:
                stats = result.get("level_stats", {}).get(level, {})
                if stats:
                    level_accuracies.append(stats.get("accuracy", 0))
            
            if level_accuracies:
                avg_level_acc = sum(level_accuracies) / len(level_accuracies)
                md_lines.append(f"| {level} | {avg_level_acc:.2%} | {len(level_accuracies)} |")
        
        md_lines.append("")
    
    # 6. å»ºè®®
    md_lines.append("## ğŸ’¡ å…³é”®å»ºè®®")
    md_lines.append("")
    recommendations = []
    recommendations.append(f"âœ… æœ€ä½³æ¨¡å‹ **{best_model['model_name']}** å‡†ç¡®ç‡ä¸º {best_model['accuracy']:.2%}ï¼Œå»ºè®®ä½œä¸ºåŸºå‡†æ¨¡å‹")
    recommendations.append(f"âš ï¸ æ¨¡å‹é—´å‡†ç¡®ç‡å·®è·ä¸º {accuracy_gap:.2%}ï¼Œå»ºè®®åˆ†æè¡¨ç°å·®å¼‚åŸå› ")
    
    if avg_accuracy < 0.5:
        recommendations.append("âš ï¸ å¹³å‡å‡†ç¡®ç‡ä½äº 50%ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–æ•°æ®è´¨é‡")
    elif avg_accuracy > 0.9:
        recommendations.append("âœ… å¹³å‡å‡†ç¡®ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ é¢˜ç›®éš¾åº¦æˆ–æ‰©å¤§æµ‹è¯„é›†è§„æ¨¡")
    
    for i, rec in enumerate(recommendations, 1):
        md_lines.append(f"{i}. {rec}")
    md_lines.append("")
    
    # 8. è¯¦ç»†æ•°æ®ï¼ˆé“¾æ¥åˆ° JSONï¼‰
    md_lines.append("---")
    md_lines.append("")
    md_lines.append("## ğŸ“ ç›¸å…³æ–‡ä»¶")
    md_lines.append("")
    md_lines.append("- å®Œæ•´æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰: `detailed_analysis_report.json`")
    md_lines.append("- æ¨¡å‹å¯¹æ¯”æ±‡æ€»: `models_comparison.xlsx`")
    md_lines.append("- å„æ¨¡å‹è¯¦ç»†æŠ¥å‘Š: `<æ¨¡å‹å>/score_report.xlsx` å’Œ `<æ¨¡å‹å>/score_report.json`")
    md_lines.append("- Word æ ¼å¼æŠ¥å‘Š: `detailed_analysis_report.docx`ï¼ˆå¦‚æœå®‰è£…äº† python-docxï¼‰")
    md_lines.append("- PDF æ ¼å¼æŠ¥å‘Š: `detailed_analysis_report.pdf`ï¼ˆå¦‚æœå®‰è£…äº† weasyprintï¼‰")
    md_lines.append("")
    
    # ä¿å­˜ Markdown æŠ¥å‘Š
    md_file = output_dir / "detailed_analysis_report.md"
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))
    
    print(f"âœ“ æ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆMarkdownï¼‰å·²ä¿å­˜: {md_file}")
    
    # å°è¯•ç”Ÿæˆ Word å’Œ PDF æ ¼å¼
    _generate_word_report(md_lines, output_dir)
    _generate_pdf_report(md_lines, output_dir)
    
    # åŒæ—¶ä¿å­˜ JSON æ ¼å¼ä½œä¸ºæ•°æ®å¤‡ä»½
    report_json = {
        "summary": {
            "total_models": summary["models_count"],
            "total_questions": summary["total_questions"],
            "evaluation_date": pd.Timestamp.now().isoformat(),
            "average_accuracy": avg_accuracy,
            "std_accuracy": float(std_accuracy),
        },
        "model_rankings": [
            {
                "rank": idx,
                "model_name": result["model_name"],
                "accuracy": result["accuracy"],
                "score": f"{result['total_score']}/{result['max_score']}",
            }
            for idx, result in enumerate(summary["results"], 1)
        ],
        "key_metrics": {
            "best_model": {
                "name": best_model["model_name"],
                "accuracy": best_model["accuracy"],
                "score": f"{best_model['total_score']}/{best_model['max_score']}",
            },
            "worst_model": {
                "name": worst_model["model_name"],
                "accuracy": worst_model["accuracy"],
                "score": f"{worst_model['total_score']}/{worst_model['max_score']}",
            },
            "accuracy_gap": accuracy_gap,
        },
        "recommendations": recommendations,
    }
    
    json_file = output_dir / "detailed_analysis_report.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(report_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ æ·±åº¦åˆ†ææŠ¥å‘Šï¼ˆJSON æ•°æ®ï¼‰å·²ä¿å­˜: {json_file}")
    
    return report_json

